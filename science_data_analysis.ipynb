{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import Counter\n",
    "\n",
    "from plotting_utils import *\n",
    "from dataloader import *\n",
    "from label_ranking import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing RFR and RPC on the fragment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383, 1024) (383, 4) (383, 4)\n"
     ]
    }
   ],
   "source": [
    "fragment_dataset = ScienceDataset(False, \"fragment\", 1)\n",
    "X_fp = fragment_dataset.X_fp\n",
    "y_yield = fragment_dataset.y_yield\n",
    "y_rank = fragment_dataset.y_ranking\n",
    "print(X_fp.shape, y_yield.shape, y_rank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 214, 2: 111, 3: 42, 1: 16})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(np.where(y_rank == 1)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always choosing Cu: 0.559\n",
      "always choosing Pd: 0.29\n"
     ]
    }
   ],
   "source": [
    "### Baseline of choosing the top two frequent best condition\n",
    "print(\"always choosing Cu:\", round(np.mean(np.reciprocal(y_rank[:,0])), 3))\n",
    "print(\"always choosing Pd:\", round(np.mean(np.reciprocal(y_rank[:,2])), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "44\n",
      "45\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "def rr(rank_true, rank_pred):\n",
    "    rr = np.array([\n",
    "        1 / rank_true[a, np.argmin(x)]\n",
    "        for a, x in enumerate(rank_pred)\n",
    "    ]).mean()\n",
    "    return rr\n",
    "\n",
    "variance_sum = 0.0\n",
    "first_diff = None\n",
    "\n",
    "rf_mrr_scores = []\n",
    "rpc_mrr_scores = []\n",
    "# Conducting the 5x2cv paired t-test as implemented in mlxtend\n",
    "for seed in range(42,47):\n",
    "    print(seed)\n",
    "    X_1, X_2, y_rank_1, y_rank_2, y_yield_1, y_yield_2 = train_test_split(X_fp, y_rank, y_yield, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    # RFR\n",
    "    gcv = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42),\n",
    "        param_grid={\"n_estimators\":[50,100,200], \n",
    "                    \"max_depth\": [5, 10, None]},\n",
    "        scoring=\"r2\",\n",
    "        n_jobs=-1,\n",
    "        cv=4\n",
    "    )\n",
    "    first_set_of_preds = []\n",
    "    second_set_of_preds = []\n",
    "    for i in range(4) :\n",
    "        gcv.fit(X_1, y_rank_1[:, i].flatten())\n",
    "        first_set_of_preds.append(gcv.predict(X_2).reshape(-1, 1))\n",
    "        gcv.fit(X_2, y_rank_2[:, i].flatten())\n",
    "        second_set_of_preds.append(gcv.predict(X_1).reshape(-1, 1))\n",
    "    y_rank_rfr_pred_1 = yield_to_ranking(np.hstack(tuple(first_set_of_preds)))\n",
    "    y_rank_rfr_pred_2 = yield_to_ranking(np.hstack(tuple(second_set_of_preds)))\n",
    "    rf_mrr1 = rr(y_rank_2, y_rank_rfr_pred_1)\n",
    "    rf_mrr2 = rr(y_rank_1, y_rank_rfr_pred_2)\n",
    "    rf_mrr_scores.extend([rf_mrr1, rf_mrr2])\n",
    "    # RPC\n",
    "    rpc1 = RPC()\n",
    "    rpc2 = RPC()\n",
    "    rpc1.fit(X_1, y_rank_1)\n",
    "    rpc_mrr1 = rr(y_rank_2, rpc1.predict(X_2))\n",
    "    rpc2.fit(X_2, y_rank_2)\n",
    "    rpc_mrr2 = rr(y_rank_1, rpc2.predict(X_1))\n",
    "    rpc_mrr_scores.extend([rpc_mrr1, rpc_mrr2])\n",
    "    # Getting statistics\n",
    "    score_diff_1 = rf_mrr1 - rpc_mrr1\n",
    "    score_diff_2 = rf_mrr2 - rpc_mrr2\n",
    "    score_mean = (score_diff_1 + score_diff_2) / 2.0\n",
    "    score_var = (score_diff_1 - score_mean) ** 2 + (score_diff_2 - score_mean) ** 2\n",
    "    variance_sum += score_var\n",
    "    if first_diff is None:\n",
    "        first_diff = score_diff_1\n",
    "\n",
    "numerator = first_diff\n",
    "denominator = np.sqrt(1 / 5.0 * variance_sum)\n",
    "t_stat = numerator / denominator\n",
    "\n",
    "pvalue = stats.t.sf(np.abs(t_stat), 5) * 2.0\n",
    "t = float(t_stat)\n",
    "p = float(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.03530002169009 5.725349637123246e-06\n",
      "RFR Mean Reciprocal Rank: 0.359\n",
      "RPC Mean Reciprocal Rank: 0.722\n"
     ]
    }
   ],
   "source": [
    "print(t, p)\n",
    "print(\"RFR Mean Reciprocal Rank:\", round(np.mean(np.array(rf_mrr_scores)), 3))\n",
    "print(\"RPC Mean Reciprocal Rank:\", round(np.mean(np.array(rpc_mrr_scores)), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
